{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing User Profiles with Spark\n",
    "#### Scott, J. A., *Getting Started with Apache Spark: From Inception to Production*, Chapter 5\n",
    "\n",
    "In this example we use a large dataset from a music streaming service to build a simple real-time dashboard that provides insight into customer behaviours."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark setup\n",
    "The following code sets up Spark, loads the modules and initialises a Spark context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully imported Spark modules\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "spark_home = \"/usr/local/spark-1.6.0-bin-hadoop2.6/\"\n",
    "sys.path.append(spark_home+\"python/\")\n",
    "sys.path.append(spark_home+\"python/lib/pyspark.zip\")\n",
    "sys.path.append(spark_home+\"python/lib/py4j-0.9-src.zip\")\n",
    "\n",
    "try:\n",
    "    from pyspark import SparkContext\n",
    "    from pyspark import SparkConf\n",
    "    from pyspark.mllib.stat import Statistics\n",
    "    print(\"Successfully imported Spark modules\")\n",
    "except ImportError as err:\n",
    "    print(\"Failed to import Spark modules:\", err)\n",
    "    sys.exit(1)\n",
    "    \n",
    "conf = SparkConf().setAppName(\"ListenerSummariser\")\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading the data\n",
    "The data for this project is in the form of CSV files that can be downloaded from:\n",
    "\n",
    "- https://raw.githubusercontent.com/mapr/mapr-demos/master/spark_music_demo/tracks.csv (events data)\n",
    "- https://raw.githubusercontent.com/mapr/mapr-demos/master/spark_music_demo/cust.csv (customer data)\n",
    "\n",
    "The music streaming service has users whom are continuously connecting to it and listening to music tracks. File `tracks.csv` consists of a collection of (simulated) customer events, one per line; each event is a client listening to a track, and the dataset spans a period of several months. The file does not include a header line.\n",
    "\n",
    "Field name | Type | Description\n",
    "-----------|------|------------\n",
    "EventID | integer | Numerical ID of the event\n",
    "CustID | integer | Numerical ID of customer\n",
    "TrackID | integer | Numerical ID of track\n",
    "EventDate | string | Date and time of the event\n",
    "mobile | integer | Indicates whether the user was listening on a mobile device\n",
    "listenzip | integer | Zip code of geolocation where track was selected\n",
    "\n",
    "The customer information dataset in file `cust.csv` consists of all statically known details about a user.\n",
    "\n",
    "Field name | Type | Description\n",
    "-----------|------|------------\n",
    "CustID | integer | Unique numerical ID of customer\n",
    "Name | string | Full name of customer\n",
    "Gender | integer | Gender of customer\n",
    "Address | string | Address of customer\n",
    "zip | integer | Zip code of customer's address\n",
    "SignDate | string | Date of addition to the service\n",
    "Status | integer | Indicates whether or not the account is active (0 = closed, 1 = active)\n",
    "Level | integer | Indicates level of service (0 = Free, 1 = Silver, 2 = Gold)\n",
    "Campaign | integer | Indicates the campaign under which the user joined:<br> **NONE**: no campaign<br> **30DAYFREE**: a '30-day free trial' offer<br> **SUPERBOWL**: a Super Bowl-related program<br> **RETAILSTORE**: an offer originating in a brick-and-mortar retail store<br> **WEBOFFER**: an offer for web-originated customers\n",
    "LinkedWithApps | integer | \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib2\n",
    "\n",
    "# Create a directory to store downloaded data (if it doesn't exist already)\n",
    "if not os.path.exists(\"source_data/\"):\n",
    "    os.makedirs(\"source_data/\")\n",
    "    \n",
    "# Download the events data file\n",
    "fname = \"tracks.csv\"\n",
    "baseurl = \"https://raw.githubusercontent.com/mapr/mapr-demos/master/spark_music_demo/\"\n",
    "url = baseurl + fname\n",
    "\n",
    "if not os.path.isfile(\"source_data/\"+fname):\n",
    "    f = urllib2.urlopen(url)\n",
    "    with open(\"source_data/\"+fname, \"wb\") as dfile:\n",
    "        dfile.write(f.read())\n",
    "        \n",
    "# Download the customers data file\n",
    "fname = \"cust.csv\"\n",
    "url = baseurl + fname\n",
    "\n",
    "if not os.path.isfile(\"source_data/\"+fname):\n",
    "    f = urllib2.urlopen(url)\n",
    "    with open(\"source_data/\"+fname, \"wb\") as dfile:\n",
    "        dfile.write(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customer analysis\n",
    "The first step is to read the CSV records with the individual track events and make a pair RDD out of all the rows. To convert each line of data into an array, first the `map()` function is used and then `reduceByKey()` is called to consolidate all the arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "trackfile = sc.textFile(\"source_data/tracks.csv\")\n",
    "\n",
    "def make_tracks_kv(str):\n",
    "    l = str.split(\",\")\n",
    "    return [l[1], [[int(l[2]), l[3], int(l[4]), l[5]]]]\n",
    "\n",
    "# Make a k,v RDD out of the input data\n",
    "tbycust = trackfile.map(lambda line: make_tracks_kv(line)).reduceByKey(lambda a,b: a+b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Individual track events are now stored in a *PairRDD*, with the customer ID as the key. A summary profile can now be computed for each user, which will include:\n",
    "\n",
    "- Average number of tracks during each period of the day (time ranges are arbitrarily defined in the code)\n",
    "- Total unique tracks, i.e. the set of unique track IDs\n",
    "- Total mobile tracks, i.e. tracks played whe the `mobile` flag was set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_stats_byuser(tracks):\n",
    "    mcount = morn = aft = eve = night = 0\n",
    "    tracklist = []\n",
    "    for t in tracks:\n",
    "        trackid, dtime, mobile, zip = t\n",
    "        if trackid not in tracklist:\n",
    "            tracklist.append(trackid)\n",
    "        d, t = dtime.split(\" \")\n",
    "        hourofday = int(t.split(\":\")[0])\n",
    "        mcount += mobile\n",
    "        if (hourofday < 5):\n",
    "            night += 1\n",
    "        elif (hourofday < 12):\n",
    "            morn += 1\n",
    "        elif (hourofday < 17):\n",
    "            aft += 1\n",
    "        elif (hourofday < 22):\n",
    "            eve += 1\n",
    "        else:\n",
    "            night += 1\n",
    "    return [len(tracklist), morn, aft, eve, night, mcount]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute profile for each user\n",
    "custdata = tbycust.mapValues(lambda a: compute_stats_byuser(a))\n",
    "\n",
    "# Compute aggregate stats for the entire track history\n",
    "aggdata = Statistics.colStats(custdata.map(lambda x: x[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling `collect()` on this RDD will persist the results back to a file. These results could be stored in a database, but here for simplicity we'll output the result to two CSV files:\n",
    "\n",
    "- `live_table.csv` which contains the latest calculations\n",
    "- `agg_table.csv` containing the aggregated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'instancemethod' object has no attribute '__getitem__'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-bcf34b1687b1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     19\u001b[0m     fwriter = csv.writer(csvfile, delimiter=' ', quotechar='|',\n\u001b[0;32m     20\u001b[0m                          quoting=csv.QUOTE_MINIMAL)\n\u001b[1;32m---> 21\u001b[1;33m     fwriter.writerow([aggdata.mean()[0], aggdata.mean[1],\n\u001b[0m\u001b[0;32m     22\u001b[0m                       \u001b[0maggdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maggdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m                       aggdata.mean()[4], aggdata.mean()[5]])\n",
      "\u001b[1;31mTypeError\u001b[0m: 'instancemethod' object has no attribute '__getitem__'"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "if not os.path.exists(\"output/\"):\n",
    "    os.makedirs(\"output/\")\n",
    "\n",
    "for k, v in custdata.collect():\n",
    "    unique, morn, aft, eve, night, mobile = v\n",
    "    tot = morn + aft + eve + night\n",
    "\n",
    "    # Persist the data, in this case write to a file    \n",
    "    with open(\"output/live_table.csv\", \"ab\") as csvfile:\n",
    "        fwriter = csv.writer(csvfile, delimiter=' ', quotechar='|',\n",
    "                             quoting=csv.QUOTE_MINIMAL)\n",
    "        fwriter.writerow([unique, morn, aft, eve, night, mobile])\n",
    "\n",
    "      \n",
    "# Do the same with the summary data\n",
    "with open(\"output/agg_table.csv\", \"wb\") as csvfile:\n",
    "    fwriter = csv.writer(csvfile, delimiter=' ', quotechar='|',\n",
    "                         quoting=csv.QUOTE_MINIMAL)\n",
    "    fwriter.writerow([aggdata.mean()[0], aggdata.mean()[1],\n",
    "                      aggdata.mean()[2], aggdata.mean()[3],\n",
    "                      aggdata.mean()[4], aggdata.mean()[5]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}